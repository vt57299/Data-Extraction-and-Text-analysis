{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba37303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pyphen # for syllable count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18c75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"Input.xlsx\"\n",
    "df = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90c6849",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ in link 11668.0\n",
      "HTTP error occurred: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ in link 17671.4\n",
      "Successfully extracted data from 112 links.\n",
      "Time taken: 311.35 seconds\n"
     ]
    }
   ],
   "source": [
    "# Data extraction\n",
    "count = 0 \n",
    "start_time = time.time()       # To check the time througout the data extraction process.\n",
    "\n",
    "# Loop through the URLs\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        # Get the URL and the URL_ID\n",
    "        url = row[\"URL\"]\n",
    "        url_id = row[\"URL_ID\"]\n",
    "\n",
    "        # Crawl and parse the web page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad responses\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the elements that contain the article title and text\n",
    "        title_element = soup.find(\"h1\", class_=\"entry-title\")\n",
    "        text_element = soup.find(\"div\",class_ = \"td-post-content tagdiv-type\")\n",
    "        \n",
    "        # Handle cases where class names are different\n",
    "        if title_element is None:\n",
    "            title_element = soup.find(\"h1\", class_=\"tdb-title-text\")\n",
    "\n",
    "        if text_element is None:\n",
    "            text_element = soup.find(\"div\", class_=\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\")\n",
    "            \n",
    "\n",
    "        if title_element is not None and text_element is not None:\n",
    "            # Extract the text content and remove any whitespace\n",
    "            title = title_element.get_text().strip()\n",
    "            text = text_element.get_text().strip()\n",
    "\n",
    "            # Save the extracted text in a text file with the URL_ID as its file name\n",
    "            output_file = f\"{url_id}.txt\"\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(title + \"\\n\")\n",
    "                f.write(text + \"\\n\")\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        else:\n",
    "            print(f\"Could not extract data from {url_id}. Check HTML structure.\")\n",
    "    \n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err} in link {url_id}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err} . in link {url_id}\")\n",
    "\n",
    "# Print the total number of successful extractions\n",
    "print(f\"Successfully extracted data from {count} links.\")\n",
    "\n",
    "# Calculate the time taken\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5690d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for stopwords, master dict and data extracted \n",
    "input_dir = \"C:/Users/VIVEK/_1_blackcoffer_project\"\n",
    "stopwords_dir = \"C:/Users/VIVEK/_1_blackcoffer_project/StopWords\"\n",
    "master_dir = \"C:/Users/VIVEK/_1_blackcoffer_project/MasterDictionary\"\n",
    "\n",
    "# Loading stop words\n",
    "stop_words = set()\n",
    "for file in os.listdir(stopwords_dir):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(stopwords_dir,file),'r') as f:\n",
    "            stop_words.update(f.read().splitlines())\n",
    "        \n",
    "# loading positive and negative words\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "for file in os.listdir(master_dir):\n",
    "    if file == 'positive-words.txt':\n",
    "        with open(os.path.join(master_dir,file),'r') as f:\n",
    "            positive_words.update(f.read().splitlines())\n",
    "    if file == 'negative-words.txt':\n",
    "        with open(os.path.join(master_dir,file),'r') as f:\n",
    "            negative_words.update(f.read().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c8f8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate each variable\n",
    "def calculate_scores(text):\n",
    "#   1. Sentimental Analysis\n",
    "    text = ' '.join(word for word in text.lower().split() if word not in stop_words)\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())   # Tokenize and convert to lowercase\n",
    "    positive_score = sum(1 for token in tokens if token in positive_words)\n",
    "    negative_score = sum(1 for token in tokens if token in negative_words)\n",
    "    \n",
    "    polarity_score = (positive_score - negative_score)/((positive_score + negative_score) +0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score)/(len(tokens) + 0.000001)\n",
    "    \n",
    "#   2. Analysis of Readability\n",
    "    sentences = nltk.sent_tokenize(text)   # list of sentences \n",
    "    total_words = len(tokens)              # Total words\n",
    "    avg_sentence_length = total_words/len(sentences) # Avg sentence length\n",
    "\n",
    "#     Percentage of Complex Words\n",
    "    complex_words = [word for word in tokens if len(word) > 2]\n",
    "    percentage_complex_words = len(complex_words)/total_words\n",
    "    \n",
    "#     Fog Index\n",
    "    fog_index = 0.4*(avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "#   3. Average Number of Words Per Sentence\n",
    "    avg_words_per_sentence = total_words/len(sentences)\n",
    "    \n",
    "#   4. Complex Word Count\n",
    "    complex_word_count = len(complex_words)\n",
    "    \n",
    "#   5. Word Count\n",
    "    cleaned_words = [word for word in tokens if word.lower() not in stop_words and re.match(r'\\w',word)]\n",
    "    word_count = len(cleaned_words)\n",
    "    \n",
    "#   6. Syllable Count Per Word --using pyphen library\n",
    "    def count_syllables(word):\n",
    "        dic = pyphen.Pyphen(lang='en-US')\n",
    "        return len(dic.inserted(word).split('-'))\n",
    "    \n",
    "    syllable_per_word = sum(count_syllables(word) for word in cleaned_words)/len(cleaned_words)\n",
    "    \n",
    "#   7. Personal Pronouns\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b',text,re.IGNORECASE))\n",
    "    \n",
    "#   8. Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in cleaned_words)/len(cleaned_words)\n",
    "    \n",
    "    return {\n",
    "        'POSITIVE SCORE' : positive_score,\n",
    "        'NEGATIVE SCORE' : negative_score,\n",
    "        'POLARITY SCORE' : polarity_score,\n",
    "        'SUBJECTIVITY SCORE' : subjectivity_score,\n",
    "        'AVG SENTENCE LENGTH' : avg_sentence_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS' : percentage_complex_words,\n",
    "        'FOG INDEX' : fog_index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE' : avg_words_per_sentence,\n",
    "        'COMPLEX WORD COUNT' : complex_word_count,\n",
    "        'WORD COUNT' : word_count,\n",
    "        'SYLLABLE PER WORD' : syllable_per_word,\n",
    "        'PERSONAL PRONOUNS' : personal_pronouns,\n",
    "        'AVG WORD LENGTH' : avg_word_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30553f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save output in excel file\n",
    "output_file = 'output Data Structure.xlsx'\n",
    "\n",
    "# Initializing a list to store computed variables\n",
    "data = []\n",
    "\n",
    "# Calculating variables by iterating through each files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(input_dir,filename)\n",
    "        url_id = os.path.splitext(filename)[0]\n",
    "        \n",
    "        with open(filepath, 'r', encoding = 'utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "# Calculating scores\n",
    "        scores = calculate_scores(content)\n",
    "        \n",
    "        row_data = [url_id] + list(scores.values())\n",
    "        data.append(row_data)\n",
    "        \n",
    "# Dataframe to store computed variables\n",
    "columns = ['URL_ID','POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "           'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', \n",
    "           'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', \n",
    "           'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', \n",
    "           'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', \n",
    "           'AVG WORD LENGTH']\n",
    "\n",
    "df = pd.DataFrame(data, columns = columns) # Computed data\n",
    "output_df = pd.read_excel(output_file)     # Output file\n",
    "\n",
    "# Merging computed data with existing data based on URL_ID\n",
    "output_df.update(df)\n",
    "\n",
    "# Saving updated output file\n",
    "output_df.to_excel('updated_output_file.xlsx', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298c766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
